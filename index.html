
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>EgoPressDiff</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content=""/>
    <meta property="og:title" content="EgoPressDiff" />
    <meta property="og:description" content="Project page for EgoPressDiff" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="EgoPressDiff" />
    <meta name="twitter:description" content="Project page for EgoPressDiff." />
    <meta name="twitter:image" content="" />


    <!--<link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>


    <style>
        .github-pill {
            border-radius: 50px;
            padding: 8px 20px;
            font-weight: 500;
            font-size: 16px;
            transition: background-color 0.3s ease;
            background-color: #000;
            color: #fff;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            text-decoration: none;
        }
        
        .github-pill:hover {
            background-color: #333;
            color: #fff;
            text-decoration: none;
        }
    </style>


</head>

<body>
    <br>
    <div class="container" id="main" style="width: 85%;">
    <!-- <div class="container" id="main"> -->
        <h2 class="col-md-8 col-md-offset-2 text-center" style="font-size: 28px; font-weight: bold; line-height: 1.5; color: #333;">
            <span style="color: #007BFF;">EgoPressDiff</span>: Multimodal Video Diffusion for Egocentric UV-Domain Hand-Pressure Estimation
        </h2>
        </div>
        <!-- Make sure you include Font Awesome -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"/>


        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <h3>
                    <em>Anonymous Submission</em>
                </h3>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2  text-center">
                <ul class="list-inline">
                    <li>
                        <a class="btn btn-dark github-pill" href="https://anonymous.4open.science/r/EgoPressDiff/README.md" target="_blank" role="button">
                            <i class="fab fa-github"></i> Code
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        
        <div class="row">
            <!-- 左侧文本部分 -->
            <div class="col-md-4 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    Abstract
                </h3>
                <p class="text-justify">
                    Estimating hand-surface contact pressure from an egocentric view is crucial for AR/VR devices, robotic imitation, and ergonomic analysis. Existing methods often discretize pressure signal and process frames independently, leading to quantization errors and temporal inconsistencies.
                    We present <b>EgoPressDiff</b>, a conditional video diffusion framework that generates UV-pressure maps from visual input.
                    The core of our approach is a multi-modal conditioning strategy, introducing a PoseNet and a Vertex Encoder to efficiently extract features from hand pose and 3D mesh vertices. These signals, along with depth information, guide the generative process to ensure the pressure fields are physically grounded. To effectively fuse these heterogeneous features, we further propose a Distribution-Calibrated Spatial Layer, which aligns their statistical properties before combination.
                    Evaluated on the EgoPressure ego-view setting, EgoPressDiff achieves state-of-the-art results, improving Volumetric IoU by over 34% relative to prior baseline, while reducing MAE and maintaining high temporal accuracy.
                </p>
            </div>
        
            <!-- 右侧图像部分 -->
            <div class="col-md-4 col-md-offset-0 text-center" style="margin-top: 30px;">
                <img width="90%" src="assets/teaser.png" class="img-responsive" alt="teaser">
                <p style="margin-top: 10px; font-size: 14px; color: #555; text-align: left;">
                    <strong>Figure 1:</strong> EgoPressDiff generates dynamic hand pressure from a single egocentric RGB video. It outputs a sequence of UV-pressure maps that could be warpped onto the 3D MANO model for intuitive 3D visualization of contact forces. The "Side View" is provided to better illustrate the hand’s contact with the touchpad.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    A. Network Architecture
                </h3>
                <p class="text-justify">
                    The training pipeline of our method is illustrated in Figure 2 (a). 
                    The network consists of several key components, including the <b>PoseNet, Vertex Encoder, and Distribution-Calibrated (DC) Spatial Layer</b>. 
                    These modules work together to extract, fuse, and align multi-modal features. 
                    First, PoseNet is employed to extract hand pose features, which are added to the input latent to explicitly enhance the model's awareness of the hand's posture. 
                    Second, since depth information is critical for inferring hand contact with the environment, 
                    we use VAE to encode the depth maps, projecting its features into the same feature space as input latents. 
                    To further capture the hand's spatial structure, 
                    we propose a Vertex Encoder that uses the 3D vertex coordinates of the MANO hand model as a geometric prior, 
                    enabling the model to learn the correlation between 3D hand geometry and pressure. Furthermore, a CLIP image encoder processes the egocentric RGB frames. 
                    The resulting image embeddings serve a dual purpose: they provide global visual context and, within the DC Spatial Layer, 
                    calibrate the distribution of hand-vertex embeddings, ensuring distributional consistency across modalities.
                    In what follows, we will present the architectural details of these key components.                 
                </p>

                <br>
                <img width="80%" src="assets/pipeline.png" class="img-responsive" alt="pipeline" style="margin:auto">
                <p style="margin-top: 10px; font-size: 14px; color: #555; text-align: left;">
                    <strong>Figure 2:</strong> Overview of EgoPressDiff.
                    <b>(a)</b> The training pipeline of EgoPressDiff. 
                    The model processes five input streams through dedicated encoders: 
                    a PoseNet for hand pose, a VAE for depth and UV-pressure, a CLIP image encoder for RGB frames, and a Vertex Encoder for hand vertices. 
                    To align features from the vertex and image embeddings, we introduce a DC Spatial Layer, which replaces the original spatial layer in the U-Net. 
                    The pipeline is trained end-to-end using a reconstruction loss, with a UV mask employed to up-weight the hand UV pixels.
                    <b>(b)</b> The architecture of Vertex Encoder.
                    <b>(c)</b> The architecture of PoseNet.
                    <b>(d)</b> Visualization of the reconstruction target, from top to bottom: The input egocentric RGB image with 3D MANO hand mesh; 
                    A 3D visualization of the ground-truth pressure on the hand surface; The pressure represented as a texture on the unwrapped UV layout;
                    The final 2D UV-pressure map, which serves as the reconstruction target. 
                </p>
                <br>

                <div style="margin-left: 40px;">
                    <h4 style="font-weight: bold;">(1) Vertex Encoder.</h4>
                    <p class="text-justify">
                        <p>
                            To extract a compact and coherent representation from MANO hand vertices, we propose a Vertex Encoder. 
                            The input is a sequence of hand-mesh vertices with shape (B, N, 778, 3), where B, N, 778, and 3 correspond to batch size, number of frames, number of vertices, and (x,y,z) coordinates. 
                            The encoder outputs a feature sequence of shape (B, N, 1024). As shown in Figure 2 (b), the process has three stages:
                            </p>
                            <ol>
                              <li>
                                <b>Spatial feature extraction:</b> Each frame is normalized to remove global translation and scale. 
                                A shared MLP maps the 3D coordinates to a higher-dimensional space, followed by symmetric max-pooling to form a frame-level embedding.
                              </li>
                              <li>
                                <b>Temporal feature mixing:</b> The embeddings are processed by depthwise-separable convolution blocks, capturing local temporal dynamics efficiently.
                              </li>
                              <li>
                                <b>Projection:</b> A linear layer with LayerNorm projects the features to the final 1024-dimensional representation.
                              </li>
                            </ol>
                            <p>
                            This hierarchical design enables efficient distillation of MANO vertex data into structured features suitable for denoising models.
                            </p>
                    </p>
                </div>
        
                <div style="margin-left: 40px;">
                    <h4 style="font-weight: bold;">(2) PoseNet.</h4>
                    <p class="text-justify">
                        Many generative models integrate human pose features via ControlNet, but this adds significant computational cost. 
                        We propose a lightweight PoseNet for extracting hand pose map features. 
                        As shown in Figure 2 (c), PoseNet consists only of convolutional and SiLU layers. 
                        For stable training, the network uses Gaussian weight initialization, and its final projection layer is a zero-initialized convolution.
                    </p>
                </div>

                <div style="margin-left: 40px;">
                    <h4 style="font-weight: bold;">(3) Distribution-Calibrated Spatial Layer.</h4>
                    <p class="text-justify">
                        <p>
                            To integrate multimodal control signals, we introduce the DC Spatial Layer. 
                            As shown in Figure 3 (a), unlike a standard diffusion U-Net spatial layer that conditions features on text, our design uses dual branches for image and vertex embeddings. 
                            Because these embeddings lie in different feature spaces, we add a calibration block to align them before fusion.
                            Let <b>z</b> be the latent input. After self-attention, <b>z</b> passes through two cross-attention blocks, yielding <b>z<sup>img</sup></b> and <b>z<sup>vtx</sup></b>. 
                            We compute their channel-wise mean and standard deviation: 
                            (<b>μ<sub>img</sub>, σ<sub>img</sub></b>) and (<b>μ<sub>vtx</sub>, σ<sub>vtx</sub></b>). 
                            We align them by enforcing:
                            \[
                            \frac{z^{img} - \mu_{img}}{\sigma_{img}} = \frac{z^{vtx} - \mu_{vtx}}{\sigma_{vtx}}
                            \]
                            From this, we derive the calibrated vertex latent:
                            \[
                            \bar{z}^{vtx} = \frac{z^{vtx} - \mu_{vtx}}{\sigma_{vtx}} \times \sigma_{img} + \mu_{img}
                            \]
                            Finally, the calibrated <b>z̄<sup>vtx</sup></b> is fused with <b>z<sup>img</sup></b> via element-wise addition and passed to the next temporal layer.
                        </p>                            
                    </p>

                    <br>
                        <img width="50%" src="assets/DCSL.png" class="img-responsive" alt="pipeline" style="margin:auto">
                        <p style="margin-top: 10px; font-size: 14px; color: #555; text-align: left;">
                            <strong>Figure 3:</strong> <b>(a)</b> The original U-Net block. <b>(b)</b> Our proposed Distribution-Calibrated (DC) Spatial Layer integrated into the U-Net block. 
                                Here, &mu; and &sigma; denote the mean and standard deviation, respectively.
                        </p>
                    <br>

                </div>
                
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    B. Training Loss
                </h3> 
                <p class="text-justify">
                    <p>
                        As illustrated in Figure 2 (a), the model is trained end-to-end with a reconstruction loss. 
                        The loss is optimized over the trainable parameters of the UNet, Vertex Encoder, and PoseNet. 
                        To enhance the physical plausibility of the output, we introduce a UV mask as a spatial prior. 
                        This mask compels the model to prioritize reconstruction accuracy within the valid hand region defined by the UV map.
                        The training objective is formulated as a weighted mean squared error:
                        \[
                        \mathcal{L} = \mathbb{E}_{\varepsilon}\left(\left\|\left(z_{gt}-z_{\varepsilon}\right)\odot\left(1+\mathbf{M}_{uv}\right)\right\|^2\right),
                        \]
                        where <b>z<sub>gt</sub></b> is the ground truth latent, <b>z<sub>ε</sub></b> is the denoised latent, and <b>M<sub>uv</sub></b> is the binary UV mask.
                    </p>                        
                </p>
            </div>
        </div>


        <div class="row"> 
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    C. Qualitative Comparison
                </h3>
                <p class="text-justify">
                    Figure 4 presents a qualitative comparison across different methods.
                    The first row illustrates a scenario where the middle finger makes contact with the surface while the index finger does not, as confirmed by the side view. 
                    Baseline methods erroneously predict pressure on the non-contacting index finger. In contrast, our method, which incorporates depth, hand geometry and temporal information, 
                    yields the correct prediction.
                    As highlighted in the red box in the second row, our diffusion-based model generates a UV-pressure map with smoother pressure transitions, closely aligning with the ground truth. 
                    This is because baseline methods typically quantize pressure into a fixed number of classes (e.g., nine) and treat the problem as a pixel-wise classification task, 
                    resulting in coarse pressure gradients.
                    Overall, our method demonstrates superior performance in terms of both the accuracy of the contact regions and the fidelity of the pressure distribution.
                </p>

                <br>
                    <img width="95%" src="assets/Comparison.png" class="img-responsive" alt="pipeline" style="margin:auto">
                    <p style="margin-top: 5px; font-size: 14px; color: #555; text-align: left;">
                        <strong>Figure 4:</strong> Qualitative comparison on the EgoPressure dataset. The "Ego View" serves as the input, 
                        while the "Side View" is provided to better illustrate the hand's contact with the touchpad. 
                        The first row shows a case where only the middle finger is in contact and only our method predicts this correctly. 
                        In the second row, magnified results of the palm (red box) demonstrate that our method produces smoother pressure transitions, more closely aligning with the ground truth (GT).
                    </p>
                <br>

            </div>
        </div>


        <div class="row"> 
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    D. Video Results
                </h3>
                <p class="text-justify">
                    We present the raw UV-pressure maps generated by EgoPressDiff, along with their corresponding visualizations projected onto the 3D MANO hand model.
                </p>


                <h4 style="font-weight: bold;">
                    case 1:
                    <p class="text-justify">
                        <!-- 以下示例展示了我们的方法对手部不同部位所受压力图像的生成效果，可以看到生成的压力图像在时间上保持了较好的连续性，并且能够准确反映手部与触摸板接触的区域和压力分布。 -->
                        Example results of our method for generating pressure maps on different regions of the hand. 
                        The generated pressure maps maintain good temporal continuity and accurately capture the contact areas and pressure distribution between the hand and the touchpad.
                    </p>
                </h4>
                <div class="row text-center">
                    <div class="row">
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_grasp-edge_uncurled_thumb-down_5x_right-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_grasp-edge_uncurled_thumb-down_5x_right-02.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_press_palm_high_x5_left-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_press_palm_high_x5_left-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_grasp-edge_curled_thumb-down_5x_right-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_grasp-edge_curled_thumb-down_5x_right-02.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_press_palm-and-fingers_high_x5_right-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_019_press_palm-and-fingers_high_x5_right-02.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <h4 style="font-weight: bold;">
                    case 2: 
                    <p class="text-justify">
                        <!-- 以下示例展示了在同一个手势动作下，手部施加的压力大小和手部与触摸板接触的区域不同的情况下，我们的方法生成的压力图像。可以看到，随着手部施加压力的变化，生成的压力图像中接触区域的颜色和分布也发生了相应的变化，反映了不同的压力值和接触模式。 -->
                        Example results under the same gesture with varying contact regions and pressure levels. 
                        As the applied pressure changes, the generated pressure maps reflect corresponding variations in color and distribution, indicating different pressure values and contact patterns.
                    </p>
                </h4>
                <div class="row text-center">
                    <div class="row">
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_021_press_fingers_high_5x_right-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_021_press_fingers_high_5x_right-02.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_021_push-away_5x_left-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_021_push-away_5x_left-02.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <h4 style="font-weight: bold;">
                    case 3: 
                    <p class="text-justify">
                        <!-- 以下示例展示了我们方法的长视频生成效果，在一个较为完整的手势动作过程中，手部与触摸板的接触区域和压力分布随着手部动作变化而变化。可以看到，生成的压力图像在时间上保持了较好的连续性，并且能够准确反映手部与触摸板接触的区域和压力分布。 -->
                        Example results of long video generation with our method. During a complete gesture, the contact areas and pressure distribution between the hand and the touchpad change over time. 
                        The generated pressure maps maintain good temporal continuity and accurately capture these dynamic variations.
                    </p>
                </h4>
                <div class="row text-center">
                    <div class="row">
                        <video id="comparison-video" width="60%" autoplay loop muted controls>
                            <source src="assets/videos/Generated_videos/p_020_press_cupped_onebyone_high_3x_right-01.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://jonbarron.info/mipnerf/">Mip-NeRF</a>.
                </p>
            </div>
        </div>

    </div>  
    <br><br><br>
</body>
</html>
